{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b06776",
   "metadata": {},
   "source": [
    "# Run Project\n",
    "This is a notebook such that models can be ran. Data not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = ''\n",
    "# Expects a df stored in pickle file (.pkl)\n",
    "MODEL_PATH = ''\n",
    "# Where to save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555f938",
   "metadata": {},
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de557dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, dataframe:pd.DataFrame, mode:str = 'train', train_test_split = [9, 1]):\n",
    "        \"\"\"\n",
    "        mode : valid values is 'test' or 'train'\n",
    "        \"\"\"\n",
    "        self.dataset = dataframe\n",
    "        self.mode = mode\n",
    "        self.train_test_split = train_test_split\n",
    "        self.last_input = None\n",
    "        self.last_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        # train 90 test 10 split\n",
    "        if self.mode == 'train':\n",
    "            return self.dataset.shape[0] // 10 * self.train_test_split[0]\n",
    "        else:\n",
    "            return self.dataset.shape[0] // 10 * self.train_test_split[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # input, target\n",
    "        input_img = self.dataset.iloc[index]['img_l']\n",
    "        target_img = self.dataset.iloc[index]['img_h']\n",
    "        input_img = input_img[np.newaxis, :]\n",
    "        target_img = target_img[np.newaxis, :]\n",
    "        # print(type(input_img))\n",
    "\n",
    "        # return input_img, target_img\n",
    "        self.last_input\n",
    "        self.last_target\n",
    "        return input_img.astype(np.float32), target_img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf832b",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Following this paper model, SRCNN: https://arxiv.org/pdf/1501.00092\n",
    "Assumptions: \n",
    "  n is the number of output channels\n",
    "  c is the number of input channels\n",
    "  k is ther kernal size aka f_1 or filter size\n",
    "SRCNN is just that, a simple CNN, \n",
    "  NO attention\n",
    "  NO skip\n",
    "  NO pooling\n",
    "  NO none ReLU functions\n",
    "Initial implementaiton has ouput image size smaller than input image size, they simply compared the center of input image to output image\n",
    "Difference in setup and loss calculation:\n",
    "  They take an image, apply blur, then the output image is smaller than the input image. This means that they take a subset of the input image for testing. For our class project set up, we take a subset of the target image instead!\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class patch(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size):\n",
    "        super(patch, self).__init__()\n",
    "        self.c1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=k_size,\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.c1(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class mapping(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size):\n",
    "        super(mapping, self).__init__()\n",
    "        self.c1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=k_size,\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.c1(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class reconstruction(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size):\n",
    "        super(reconstruction, self).__init__()\n",
    "        self.c1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=k_size,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.c1(x)\n",
    "        return out\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, f1=9, f2=1, f3=5, n1=64, n2=32, img_channel=1):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.part1 = patch(img_channel, n1, f1)\n",
    "        self.part2 = mapping(n1, n2, f2)\n",
    "        self.part3 = reconstruction(n2, img_channel, f3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.interpolate(x, size=(179, 221), mode='bicubic')\n",
    "        out = self.part1(out)\n",
    "        out = self.part2(out)\n",
    "        out = self.part3(out)\n",
    "        return out\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.forward(*args, **kwds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac051c72",
   "metadata": {},
   "source": [
    "## Train / Test Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b6dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN_loss():\n",
    "    def __init__(self):\n",
    "        super(SRCNN_loss, self).__init__()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        h_diff = abs(x.shape[2] - y.shape[2])\n",
    "        w_diff = abs(x.shape[3] - y.shape[3])\n",
    "        w_margin = w_diff // 2\n",
    "        h_margin = h_diff // 2\n",
    "        y_sub_img = y[:, :, w_margin:(y.shape[2] - w_margin) , h_margin:(y.shape[3] - h_margin)]\n",
    "        return torch.square(y_sub_img - x).sum()\n",
    "    \n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.forward(*args, **kwds)\n",
    "\n",
    "def train_loop(dataloader, device, model, loss_fn, optimizer, batch_size):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X.to(device))\n",
    "        # loss = loss_fn(pred, y.to(device))\n",
    "        loss = loss_fn.forward(pred, y.to(device))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch % 2 == 0: \n",
    "            # Keep in mind how many batchs (number of enumerate) is tied to batch size in dataloader\n",
    "            loss, current = loss.item(), batch * batch_size + X.shape[0]\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, device, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.to(device))\n",
    "            test_loss += loss_fn(pred, y.to(device)).item()\n",
    "            # correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f7c24",
   "metadata": {},
   "source": [
    "## Test / Train Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# model = ImageUpscaler(scale_factor=2, num_channels=1, num_residual_blocks=8, base_channels=64)\n",
    "# model = UpsampleCNN()\n",
    "model = SRCNN()\n",
    "\n",
    "# Get Data\n",
    "dataset = pd.read_pickle(DATASET_PATH)\n",
    "input_image = dataset.loc[0, 'img_l']\n",
    "train_dataset = myDataset(dataset)\n",
    "test_dataset = myDataset(dataset, 'test')\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training setup \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function (common choices for super-resolution)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = SRCNN_loss() #SRCNN Loss as in paper\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, device, model, criterion, optimizer, batch_size)\n",
    "    test_loop(test_loader, device, model, criterion)\n",
    "print(\"Done!\")\n",
    "torch.save(model.state_dict(), os.path.join(MODEL_PATH, 'model1.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
